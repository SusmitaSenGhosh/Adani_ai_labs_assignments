# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YceNZYxOP46K8qrW2aQUTCRT9HvpKBDV
"""

# prompt: video stitching

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import imutils

import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import imutils

def extract_frames(video_path, step=1):
    """Extracts every 'step' frame from the video."""
    cap = cv2.VideoCapture(video_path)
    frames = []
    frame_id = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_id % step == 0:
            frames.append(frame)
        frame_id += 1

    cap.release()
    return frames

def get_kp_des(img1,img2):
    sift = cv2.SIFT_create(nfeatures=5000,nOctaveLayers = 5)
    kp1, desc1 = sift.detectAndCompute(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), None)
    kp2, desc2 = sift.detectAndCompute(cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY), None)
    # kp1, desc1 = sift.detectAndCompute(img1, None)
    # kp2, desc2 = sift.detectAndCompute(img2, None)
    return kp1,desc1,kp2,desc2

def get_kp_des_reg(img1,img2):
    sift = cv2.SIFT_create(nfeatures=5000)

    step_size = 10  # Adjust step size for denser coverage
    keypoints = [cv2.KeyPoint(x, y, step_size) for y in range(0, img1.shape[0], step_size)
                                                  for x in range(0, img1.shape[1], step_size)]

    # Compute descriptors at these keypoints
    kp1, desc1 = sift.compute(img1, keypoints)

    keypoints = [cv2.KeyPoint(x, y, step_size) for y in range(0, img2.shape[0], step_size)
                                                  for x in range(0, img2.shape[1], step_size)]

    # Compute descriptors at these keypoints
    kp2, desc2 = sift.compute(img2, keypoints)

    return kp1,desc1,kp2,desc2

def matching_kp(desc1,decs2):
    bf = cv2.BFMatcher()
    matches = bf.knnMatch(desc1, desc2, k=2)
    good_matches = []
    for m, n in matches:
        if m.distance < .75* n.distance:
            good_matches.append(m)
    return good_matches

def get_matching_kp(kp1,kp2):
    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    return src_pts,dst_pts

def homography_compute(kp1,kp2):
    src_pts,dst_pts = get_matching_kp(kp1,kp2)
    H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
    print(H)
    return H

# def blend_with_mask(img2_warped, img1_translated):
#     gray_warped = cv2.cvtColor(img1_translated, cv2.COLOR_BGR2GRAY)
#     _, mask_warp = cv2.threshold(gray_warped, 1, 255, cv2.THRESH_BINARY)
#     mask_warp = mask_warp.astype(np.float32) / 255.0
#     mask_warp = cv2.merge([mask_warp, mask_warp, mask_warp])
#     kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))
#     mask_warp = cv2.erode(mask_warp, kernel, iterations=1)
#     mask_img1 = 1.0 - mask_warp

#     warped_img2_float = img2_warped.astype(np.float32)
#     img1_float = img1_translated.astype(np.float32)

#     blended = warped_img2_float * mask_warp + img1_float * mask_img1
#     blended = np.clip(blended, 0, 255).astype(np.uint8)
#     # mask = (img2 > 0)  # Avoid black regions
#     # img1_warped[mask] = img2[mask]
#     return blended

def blend_with_mask(img_translated,img_warped):
    gray_warped = cv2.cvtColor(img_warped, cv2.COLOR_BGR2GRAY)
    _, mask_warp = cv2.threshold(gray_warped, 1, 255, cv2.THRESH_BINARY)
    mask_warp = mask_warp.astype(np.float32) / 255.0
    mask_warp = cv2.merge([mask_warp, mask_warp, mask_warp])
    # kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))
    # mask_warp = cv2.erode(mask_warp, kernel, iterations=1)
    mask_translated = 1.0 - mask_warp

    img_warped_float = img_warped.astype(np.float32)
    img_translated_float = img_translated.astype(np.float32)

    # blended = warped_img2_float * mask_warp + img1_float * mask_img1
    blended = img_warped_float * mask_warp + img_translated_float * mask_translated
    blended = np.clip(blended, 0, 255).astype(np.uint8)
    # mask = (img2 > 0)  # Avoid black regions
    # img1_warped[mask] = img2[mask]
    return blended

def warp_with_full_view(img1, img2, H):
    """Warp img1 using homography H while preserving all content (avoiding cropping)."""
    h1, w1 = img1.shape[:2]
    h2, w2 = img2.shape[:2]

    # Define corner points of img1
    corners_img1 = np.array([
        [0, 0], [w1, 0], [w1, h1], [0, h1]
    ], dtype=np.float32).reshape(-1, 1, 2)

    # Transform the corners using the homography
    corners_transformed = cv2.perspectiveTransform(corners_img1, H)

    # Get the bounding box of the transformed image
    x_min = min(corners_transformed[:, 0, 0].min(), 0)
    x_max = max(corners_transformed[:, 0, 0].max(), w2)
    y_min = min(corners_transformed[:, 0, 1].min(), 0)
    y_max = max(corners_transformed[:, 0, 1].max(), h2)

    # Convert float indices to integers
    x_min, x_max, y_min, y_max = map(int, [x_min, x_max, y_min, y_max])

    # Compute the translation matrix to shift everything into view
    translation = np.array([
        [1, 0, -x_min],
        [0, 1, -y_min],
        [0, 0, 1]
    ])

    # Update homography with translation
    H_translated = np.dot(translation, H)

    # New output size
    new_width = x_max - x_min
    new_height = y_max - y_min

    # Warp the image with the new homography
    img1_warped = cv2.warpPerspective(img1, H_translated, (new_width, new_height))
    # print("img2_warped")
    # cv2_imshow(img2_warped)
    # print("img1")
    # cv2_imshow(img1)
    # Place img2 in the correct position
    img2_translated = np.zeros_like(img1_warped)
    img2_translated[-y_min:h2-y_min, -x_min:w2-x_min] = img2  # Ensure integer indices

    # Blend images

    stitched_image = blend_with_mask(img1_warped,img2_translated)

    return stitched_image
def remove_black_border(image):
    """Removes black borders from an image by finding the largest non-black area."""
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply binary thresholding (black pixels = 0, non-black = 255)
    _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)

    # Find contours of the non-black region
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        # Get bounding box of the largest contour
        x, y, w, h = cv2.boundingRect(contours[0])
        cropped_image = image[y:y+h, x:x+w]
        return cropped_image
    else:
        return image

# ... (rest of your code) ...


video_path = "/content/drive/MyDrive/panorama_gen/video_input_panorama2.mp4"
frames = extract_frames(video_path, step=1)
print('total number of frames: ', len(frames))
img1 = frames[0]
for i in range(1,len(frames)):
    print(i)
    img2 = frames[i]
    # cv2_imshow(img1)
    # cv2_imshow(img2)
    kp1,desc1,kp2,desc2 = get_kp_des_reg(img1,img2)
    good_matches =  matching_kp(desc1,desc2)
    # img3 = cv2.drawMatches(img1,kp1,img2,kp2,good_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    # cv2_imshow(img3)

    # cv2_imshow(warped_img1)
    # cv2_imshow(img2)
    H = homography_compute(kp1,kp2)
    # blended= warp(img1, img2, H)
    blended= warp_with_full_view(img1, img2, H)
    blended = remove_black_border(blended)
    img1 = blended
    # print('merged image')
cv2_imshow(blended)
cv2.imwrite("/content/drive/MyDrive/panorama_gen/panorama_final.jpg", blended)