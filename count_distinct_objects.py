# -*- coding: utf-8 -*-
"""Count_distinct_objects.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18F0NZn0eSKukJ8cCgPqQzxK7285Uda8d
"""

!pip install ultralytics

import cv2
import numpy as np
from ultralytics import YOLO
from google.colab.patches import cv2_imshow
from skimage.feature import local_binary_pattern
from scipy.spatial.distance import euclidean


video_path = "./inputs/input_object_count_video.mp4" # change to the video path
output_path = "/outputs/output_object_count_video.mp4" # change to the video path


def compute_lbp_histogram(image, P=8, R=1, method="uniform"):
    """ Compute LBP histogram for an image """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
    lbp = local_binary_pattern(gray, P, R, method)  # Compute LBP
    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, P + 3), density=True)  # Compute histogram
    return hist


def compare_histograms(hist1, hist2):
    """ Compare histograms using euclidean distance metrics """
    return euclidean(hist1, hist2)

def extract_features(image,descriptor):
    # Feature extractor (SIFT)
    sift = cv2.SIFT_create(nOctaveLayers = 5,edgeThreshold = 20)
    if descriptor == 'SIFT':
      """Extracts SIFT features from an image."""
      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
      keypoints, descriptors = sift.detectAndCompute(gray, None)
    elif descriptor == 'ORB':
      orb = cv2.ORB_create()
      keypoints, descriptors = orb.detectAndCompute(image, None)
    return descriptors


def match_features(desc1, desc2):
    """Matches features using BFMatcher and returns a similarity score."""
    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
    if desc1 is None or desc2 is None:
        return 0
    matches = bf.match(desc1, desc2)
    return len(matches)


# Load the YOLOv8 model for object detection
model = YOLO("yolov8s.pt")  # Use a pretrained YOLO model

# Open the video file
cap = cv2.VideoCapture(video_path)

# Get video properties
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
fps = int(cap.get(cv2.CAP_PROP_FPS))

# output video
out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))

# Store detected objects (format: {id: (bbox, features)})
tracked_objects = {}
next_id = 0
frame_count = 0
skip_frames = 1
# Process video frames

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: # if no frame avialable do not prcees
        break
    frame_count += 1

    if frame_count % skip_frames != 0:
        continue

    # Detect objects in the current frame
    results = model(frame)
    # print('results_count', len(results))
    # for result in results:
        # print('i am here')
    for box in results[0].boxes.xyxy:  # Bounding boxes
        x1, y1, x2, y2 = map(int, box[:4])
        # print(x1, y1, x2, y2,frame_width,frame_height)
        object_width = x2 - x1
        object_height = y2 - y1
        object_area = object_width * object_height
        image_area = frame_width * frame_height

        # Check if object size is greater than 75% of the image
        if (object_area / image_area > 0.75):
            continue
        if x1 <= 1 or y1 <= 1 or x2 >= frame_width-2 or y2 >= frame_height-2:
            continue

        obj_crop = frame[y1:y2, x1:x2]  # Crop detected object

        # cv2_imshow(obj_crop)
        # Extract features
        obj_features = extract_features(obj_crop,'SIFT')
        # obj_features = compute_lbp_histogram(obj_crop)

        # for the first frame all objects are distinct object
        # print("framecount",frame_count)
        if frame_count == skip_frames:
            # print("i am here ", frame_count)
            tracked_objects[next_id] = (box, obj_features)
            matched_id  = next_id
            next_id = next_id + 1
            # print(tracked_objects)
        else:
          matched_id = None
          best_score = 1
          # Compare with existing objects
          for obj_id, (bbox, features) in tracked_objects.items():
              score = match_features(obj_features, features)
              # print(obj_id,obj_features.shape[0],features.shape[0],((obj_features.shape[0]+features.shape[0])/2)*0.4,score)
              if score > best_score and score >=((obj_features.shape[0]+features.shape[0])/2)*0.3:  # Set a threshold for matching
                  best_score = score
                  matched_id = obj_id

          if matched_id is None:
              # Assign a new ID
              tracked_objects[next_id] = (box, obj_features)
              matched_id = next_id
              next_id += 1

        # Draw bounding box and ID
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, f"ID: {matched_id}", (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    # print(tracked_objects)
    out.write(frame)
    # Show the frame with annotations
    cv2_imshow(frame)
    print("####################################################")


cap.release()
out.release()

print(f"Total Unique Objects Detected: {len(tracked_objects)}")